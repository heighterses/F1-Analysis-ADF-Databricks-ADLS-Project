# F1-Analysis-Databricks-ADLS-Data-Engineering-Project-Azure
Analyzed F1 race data (1950s-2021) using Databricks for transformations and Azure Data Lake Storage Gen2 for storage in fully automated Azure Data Factory pipelines.

![Data Source](https://github.com/user-attachments/assets/a6af8eec-a653-4009-aace-4b36d8669c1d)

## Project Overview: 
This project covers the entire journey from raw data to valuable insights that can be utilized by machine learning engineers, data scientists, and analysts to produce meaningful results.

### ğ—§ğ—²ğ—°ğ—µğ—»ğ—¼ğ—¹ğ—¼ğ—´ğ—¶ğ—²ğ˜€ ğ—¨ğ˜€ğ—²ğ—± ğŸ‘©ğŸ»â€ğŸ’»:

#### ğ——ğ—®ğ˜ğ—®ğ—¯ğ—¿ğ—¶ğ—°ğ—¸ğ˜€: For processing data, from raw ingestion to final transformations.
#### ğ—”ğ˜‡ğ˜‚ğ—¿ğ—²: For managing cloud resources.
#### ğ—”ğ˜‡ğ˜‚ğ—¿ğ—² ğ——ğ—®ğ˜ğ—® ğ—™ğ—®ğ—°ğ˜ğ—¼ğ—¿ğ˜†: To orchestrate data pipelines and automate processes.
#### ğ—”ğ˜‡ğ˜‚ğ—¿ğ—² ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—®ğ—¸ğ—² ğ—¦ğ˜ğ—¼ğ—¿ğ—®ğ—´ğ—² ğ—šğ—²ğ—»ğŸ®: For creating containers and storing files in formats such as JSON, CSV, Parquet, and Delta.
#### ğ—”ğ˜‡ğ˜‚ğ—¿ğ—² ğ—ğ—²ğ˜† ğ—©ğ—®ğ˜‚ğ—¹ğ˜: To securely manage storage account keys and integrate them with Databricks.
#### ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸: For data processing.
#### ğ—¦ğ—¤ğ—Ÿ: For querying the data.
#### ğ——ğ—²ğ—¹ğ˜ğ—® ğ—Ÿğ—®ğ—¸ğ—²: ensures data reliability, supports ACID transactions, and enhances data quality, making it ideal for building scalable and efficient data lakehouses.

## ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ—ªğ—¼ğ—¿ğ—¸ğ—³ğ—¹ğ—¼ğ˜„ ğŸ› ï¸:

### ğ——ğ—®ğ˜ğ—® ğ—–ğ—¼ğ—¹ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» ğŸ“¦ :
Fetched structured and semi-structured data (CSV, single-line JSON, multi-line JSON) from the Ergast API and stored it in Azure Data Lake Storage Gen2.
![Screenshot (96)](https://github.com/user-attachments/assets/6699efa4-9605-46c1-acd2-e661ece738cb)


### ğ——ğ—®ğ˜ğ—® ğ—œğ—»ğ—´ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—» ğŸ”¢:
Connected Databricks to the storage location and ingested the data from ADLS to Databricks.
![Screenshot (97)](https://github.com/user-attachments/assets/e57a08be-a730-4e49-8577-4a96ca8ebb5e)


### ğ——ğ—®ğ˜ğ—® ğ—©ğ—®ğ—¹ğ—¶ğ—±ğ—®ğ˜ğ—¶ğ—¼ğ—» âœ…:
Created multiple notebooks in Databricks for validating and processing different files.
![Screenshot (98)](https://github.com/user-attachments/assets/77ae2408-af70-45db-a839-f386f3eba4af)


### ğ——ğ—®ğ˜ğ—® ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ˜ğ—¶ğ—¼ğ—» ğŸ”:
Applied transformations and stored the processed files in the transformation layer on Azure Data Lake Storage in Delta format.
![Screenshot (99)](https://github.com/user-attachments/assets/60ee253d-bfd1-49b5-a4fc-c9341cb3dd35)


### ğ——ğ—®ğ˜ğ—® ğ—£ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ğ—¶ğ—»ğ—´ âš™ï¸:
Processed the transformed data using PySpark and PySpark SQL to extract meaningful results.
![Screenshot (100)](https://github.com/user-attachments/assets/e168ed81-afef-4cdb-8a0d-ea822ec85f31)


### ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—² ğ—–ğ—¿ğ—²ğ—®ğ˜ğ—¶ğ—¼ğ—» ğŸ›¢ï¸:
Created a database on Azure Data Lake Storage to store the processed data in tables.
![Screenshot (101)](https://github.com/user-attachments/assets/1d2c614e-576e-46ba-9704-0d5c4304c88f)


### ğ—¦ğ˜ğ—¼ğ—¿ğ—®ğ—´ğ—² ğŸ—‚ï¸:
Stored all tables in the processed layer (container) on Azure Data Lake Storage in ğ——ğ—²ğ—¹ğ˜ğ—® ğ—™ğ—¼ğ—¿ğ—ºğ—®ğ˜.
![Screenshot (102)](https://github.com/user-attachments/assets/99016342-f799-4d11-ac48-d514581b24c0)



### ğ—£ğ—¿ğ—²ğ˜€ğ—²ğ—»ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—Ÿğ—®ğ˜†ğ—²ğ—¿ ğŸ¯:
Developed a presentation layer using SQL to join different tables and provide visual analytics.
![Screenshot (91)](https://github.com/user-attachments/assets/7f938f7a-63d0-404d-ad5d-c8669a2e780e)


### ğ—œğ—»ğ˜ğ—²ğ—´ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ˜„ğ—¶ğ˜ğ—µ ğ—£ğ—¼ğ˜„ğ—²ğ—¿ ğ—•ğ—œ ğŸ“Š:
Connected the presentation layer to Power BI, enabling data analysts to perform further analysis and visualization.
![Screenshot (103)](https://github.com/user-attachments/assets/6946853c-17ee-4e42-b27b-9d96064700da)

##### *P.S.: The connection with Power BI hasn't been completed yet.*

### ğ—”ğ˜‚ğ˜ğ—¼ğ—ºğ—®ğ˜ğ—¶ğ—¼ğ—» ğŸ¤–:
Utilized ğ—”ğ˜‡ğ˜‚ğ—¿ğ—² ğ——ğ—®ğ˜ğ—® ğ—™ğ—®ğ—°ğ˜ğ—¼ğ—¿ğ˜† ğ˜ğ—¼ ğ—°ğ—¿ğ—²ğ—®ğ˜ğ—² ğ—® ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² that automates the entire process. When new data is added to the raw container, the pipeline is triggered automatically, processing the data through all the engineering steps.
![Screenshot (95)](https://github.com/user-attachments/assets/47dd0f0f-8f06-4eb3-9799-eb1d8dd42ace)


# Thank You for Consideration :)

